import os
import json
import faiss
import numpy as np
from typing import List, Optional
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer

# --- GEMINI & KRÄ°Z MODÃœLÃœ ---
import google.generativeai as genai
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

# ==========================================
# ğŸ”‘ API KEY AYARI
# (Kendi anahtarÄ±nÄ± tÄ±rnak iÃ§ine yapÄ±ÅŸtÄ±r)
# ==========================================
GEMINI_API_KEY = ""

# --- AYARLAR ---
VECTOR_STORE_DIR = "data/vector_store"
MODEL_NAME = 'paraphrase-multilingual-mpnet-base-v2'
SENTIMENT_MODEL_ID = "savasy/bert-base-turkish-sentiment-cased"

app = FastAPI(title="Psikoloji AI Chatbot API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global DeÄŸiÅŸkenler
embedding_model = None
index = None
chunk_map = None
sentiment_tokenizer = None
sentiment_model = None

@app.on_event("startup")
def load_resources():
    global embedding_model, index, chunk_map, sentiment_tokenizer, sentiment_model
    print("ğŸš€ SÄ°STEM BAÅLATILIYOR...")
    
    # 1. Embedding Model (CPU - BilgisayarÄ± yormaz)
    print("ğŸ“¦ 1. Embedding Modeli (CPU) YÃ¼kleniyor...")
    embedding_model = SentenceTransformer(MODEL_NAME, device='cpu')
    
    try:
        index = faiss.read_index(os.path.join(VECTOR_STORE_DIR, "vector_store.index"))
        with open(os.path.join(VECTOR_STORE_DIR, "chunk_map.json"), 'r', encoding='utf-8') as f:
            chunk_map = json.load(f)
        print("âœ… RAG VeritabanÄ± HazÄ±r!")
    except Exception as e:
        print(f"âŒ RAG YÃ¼kleme HatasÄ±: {e}")

    # 2. Kriz Modeli (CPU)
    try:
        print("ğŸ“¦ 2. Kriz Modeli (CPU) YÃ¼kleniyor...")
        sentiment_tokenizer = AutoTokenizer.from_pretrained(SENTIMENT_MODEL_ID)
        sentiment_model = AutoModelForSequenceClassification.from_pretrained(SENTIMENT_MODEL_ID).to("cpu")
        print("âœ… Kriz Modeli HazÄ±r!")
    except Exception as e:
        print(f"âŒ Kriz Modeli HatasÄ±: {e}")

# --- GELÄ°ÅMÄ°Å KRÄ°Z TESPÄ°TÄ° (Filtreli) ---
def detect_crisis(text):
    if not sentiment_model or not sentiment_tokenizer:
        return False, 0.0

    # 1. ADIM: HIZLI FÄ°LTRE (Keywords)
    # EÄŸer bu kelimeler yoksa, modeli boÅŸuna Ã§alÄ±ÅŸtÄ±rma ve alarm verme.
    risk_keywords = [
        "Ã¶lmek", "intihar", "canÄ±ma kÄ±y", "dayanamÄ±yorum", "bÄ±ktÄ±m", "hap iÃ§", 
        "kendimi kes", "yaÅŸamak istemiyorum", "her ÅŸey bitsin", "veda", 
        "artÄ±k son", "kimse beni sevmiyor", "kurtulmak istiyorum"
    ]
    
    text_lower = text.lower()
    keyword_hit = any(word in text_lower for word in risk_keywords)

    # EÄŸer riskli kelime HÄ°Ã‡ yoksa, direkt gÃ¼venli kabul et.
    if not keyword_hit:
        return False, 0.0

    # 2. ADIM: DERÄ°N ANALÄ°Z (Model)
    # Sadece riskli kelime varsa buraya girer.
    inputs = sentiment_tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    inputs = {key: val.to("cpu") for key, val in inputs.items()}

    with torch.no_grad():
        logits = sentiment_model(**inputs).logits
    
    probabilities = torch.softmax(logits, dim=1)
    
    # savasy modelinde genelde: Index 0 -> Negatif, Index 1 -> Pozitif olabilir
    # Ancak biz keyword kontrolÃ¼ yaptÄ±ÄŸÄ±mÄ±z iÃ§in sadece negatif skora bakacaÄŸÄ±z.
    # Genelde Index 0 negatiftir bu modelde.
    negative_score = probabilities[0][0].item() 

    print(f"ğŸ” Kriz Analizi: '{text}' | Kelime: Var | Negatiflik: {negative_score:.4f}")

    # KURAL: Hem kelime geÃ§ecek HEM DE model %70 Ã¼stÃ¼ negatif diyecek.
    # Veya kelime Ã§ok net "intihar" ise skora bakmadan uyar.
    is_crisis = False
    
    if negative_score > 0.70:
        is_crisis = True
    elif "intihar" in text_lower or "Ã¶lmek" in text_lower:
        is_crisis = True
        
    return is_crisis, negative_score

# --- VERÄ° MODELLERÄ° ---
class Message(BaseModel):
    role: str
    content: str

class UserProfile(BaseModel):
    name: str = "KullanÄ±cÄ±"
    age: int = 0
    gender: str = "Belirtilmedi"

class ChatRequest(BaseModel):
    query: str
    history: List[Message] = []
    user_profile: Optional[UserProfile] = None
    k: int = 3

@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    # 1. KRÄ°Z KONTROLÃœ
    is_crisis, confidence = detect_crisis(request.query)
    
    if is_crisis:
        print(f"ğŸš¨ KRÄ°Z TESPÄ°T EDÄ°LDÄ°! Skor: {confidence:.4f}")
        return {
            "reply": (
                "âš ï¸ **Ã–NEMLÄ° UYARI:** YazdÄ±klarÄ±nÄ±zdan zor bir sÃ¼reÃ§ten geÃ§tiÄŸiniz anlaÅŸÄ±lÄ±yor. "
                "LÃ¼tfen yalnÄ±z kalmayÄ±n.\n\n"
                "**Acil Destek:**\n"
                "- ğŸ“ **112** Acil Ã‡aÄŸrÄ±\n"
                "- ğŸ“ **ALO 183** Sosyal Destek"
            ),
            "sources": ["KRÄ°Z PROTOKOLÃœ"],
            "is_crisis": True 
        }

    # 2. RAG ARAMASI
    try:
        query_vector = embedding_model.encode([request.query])
        distances, indices = index.search(np.array(query_vector).astype('float32'), request.k)

        retrieved_texts = []
        sources = []
        if chunk_map:
            for i, idx in enumerate(indices[0]):
                if idx == -1: continue
                try:
                    # JSON keyleri string olabilir, int'e Ã§eviriyoruz veya tam tersi
                    chunk_file = chunk_map[str(idx)] if str(idx) in chunk_map else chunk_map[idx]
                    
                    with open(chunk_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        retrieved_texts.append(f"- {data[0]['text']}")
                        sources.append(os.path.basename(chunk_file))
                except: continue
        
        context_block = "\n".join(retrieved_texts)
    except Exception as e:
        print(f"RAG HatasÄ±: {e}")
        context_block = ""

    # 3. GEMINI HAZIRLIÄI
    genai.configure(api_key=GEMINI_API_KEY)
    
    profile_text = ""
    if request.user_profile:
        p = request.user_profile
        profile_text = f"KULLANICI PROFÄ°LÄ°: AdÄ±: {p.name}, YaÅŸÄ±: {p.age}, Cinsiyeti: {p.gender}."

    system_instruction = f"""
    Sen BiliÅŸsel DavranÄ±ÅŸÃ§Ä± Terapi (BDT) konusunda uzman, empatik bir yapay zeka psikoloji asistanÄ±sÄ±n.
    
    {profile_text}
    
    AÅAÄIDAKÄ° KAYNAK BÄ°LGÄ°LERÄ° (CONTEXT) KULLANARAK CEVAP VER:
    {context_block}

    KURALLAR:
    1. KullanÄ±cÄ±ya ismiyle hitap et ve "sen" dili kullan.
    2. Context iÃ§indeki bilimsel bilgileri sohbetin iÃ§ine doÄŸalca yedir.
    3. KullanÄ±cÄ±ya tavsiye vermek yerine, onu dÃ¼ÅŸÃ¼ndÃ¼recek sorular sor (Sokratik Sorgulama).
    4. Samimi ve kÄ±sa tut.
    5. CevaplarÄ±nda "Yapay zeka", "Dil modeli", "Bilgi kesilme tarihi" gibi robotik ifadeler KULLANMA.
    """

    # Model Ä°smi DÃ¼zeltildi: gemini-2.5-flash
    model = genai.GenerativeModel(
        'gemini-2.5-flash',
        system_instruction=system_instruction
    )

    gemini_history = []
    for msg in request.history:
        role = 'user' if msg.role == 'user' else 'model'
        gemini_history.append({'role': role, 'parts': [msg.content]})

    try:
        chat = model.start_chat(history=gemini_history)
        response = chat.send_message(request.query)
        ai_reply = response.text
    except Exception as e:
        ai_reply = f"BaÄŸlantÄ± hatasÄ± oluÅŸtu: {str(e)}"

    return {"reply": ai_reply, "sources": list(set(sources)), "is_crisis": False}
